cluster:
  logdir=$(dirname workflow/logs/{rule}/{rule}-{wildcards}) &&
  mkdir -p $logdir &&
  sbatch
    --account=biohub
    --partition=skylake
    --nodes=1
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --time={resources.runtime}
    --job-name={rule}-{wildcards}
    --output=workflow/logs/{rule}/{rule}-{wildcards}-%j.log
    --parsable
default-resources:
  - mem_mb=1000
  - runtime="1h"
restart-times: 1
jobs: 100
max-jobs-per-second: 2
max-status-checks-per-second: 5
local-cores: 1
use-conda: True
conda-prefix: "/hpcfs/users/a1018048/envs/"
latency-wait: 60
notemp: True
keep-going: True
rerun-incomplete: True
rerun-triggers: mtime
printshellcmds: True
scheduler: greedy
cluster-cancel: scancel
notemp: False  ## If storage quotas permit, I prefer to hold temp files and use --delete-temp-output upon completion